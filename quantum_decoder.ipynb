{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a91d19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== OUTPUT (GPT + Quantum Attention Gate) ===\n",
      " Hello from a quantum‑enhanced decoder.\n",
      "\n",
      "As a quantum‑enhanced decoder, you're able to apply a source-of-energy law to the lattice, generating a large set of entangled states in which the quantum-enhanced decoder (or, for that matter, the source of a different-state state in which the source\n"
     ]
    }
   ],
   "source": [
    "# gpt2_quantum_adapter.py\n",
    "\"\"\"\n",
    "Goal: Use a GPT‑style decoder (pretrained GPT‑2 by default) and inject a *small but real*\n",
    "quantum component that influences generation, while keeping everything stable & fast.\n",
    "\n",
    "Design: **Quantum Attention Gate (QAG)**\n",
    "- At each transformer block, we compute a tiny summary vector of the current hidden states.\n",
    "- Convert that summary to PQC angles, run a very small PennyLane circuit (e.g., 4–8 qubits).\n",
    "- The circuit outputs a vector that *scales each attention head's output* (per block).\n",
    "- This is lightweight: exactly **one PQC call per block per forward** (not per token per head).\n",
    "\n",
    "Pros:\n",
    "- Works with *pretrained* GPT‑2 without breaking its shapes.\n",
    "- Keeps sampling speed reasonable for token‑by‑token generation.\n",
    "- Gives you a clean quantum knob that can be trained end‑to‑end.\n",
    "\n",
    "Notes:\n",
    "- PQC runs on CPU by default; classical parts run on CUDA.\n",
    "- Trainable parameters live mostly in the small linear projections + PQC weights.\n",
    "- You can freeze the LM weights and train only the quantum adapter, or fine‑tune both.\n",
    "\n",
    "Usage\n",
    "-----\n",
    "conda create -n qdec python=3.11 -y\n",
    "conda activate qdec\n",
    "conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia -y\n",
    "pip install transformers accelerate pennylane sentencepiece\n",
    "\n",
    "# Quick generation with GPT‑2 + quantum gate (randomly initialized gate)\n",
    "python gpt2_quantum_adapter.py --model gpt2 --prompt \"Once upon a time\" --max_new_tokens 64\n",
    "\n",
    "# Optional: train the quantum gate (freeze GPT‑2) on a small LM task (next-token LM) — coming soon.\n",
    "\"\"\"\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pennylane as qml\n",
    "from pennylane.templates import StronglyEntanglingLayers\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class QAGConfig:\n",
    "    num_qubits: int = 6\n",
    "    q_layers: int = 2\n",
    "    reduce: str = \"mean\"  # how to summarize hidden states: mean | cls | last\n",
    "    heads_per_block: Optional[int] = None  # auto-infer for GPT‑2\n",
    "    scale_range: float = 0.5  # scale in [1-scale, 1+scale]\n",
    "    diff_method: str = \"backprop\"  # pennylane diff\n",
    "    device_name: str = \"default.qubit\"\n",
    "\n",
    "\n",
    "class QuantumAttentionGate(nn.Module):\n",
    "    \"\"\"Quantum gate that outputs per‑head scales given a block summary vector.\n",
    "\n",
    "    Steps: summary -> proj -> angles -> PQC -> linear -> per‑head scales in [1-s, 1+s].\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim: int, n_heads: int, cfg: QAGConfig):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.n_heads = n_heads\n",
    "        self.to_angles = nn.Linear(in_dim, cfg.num_qubits)\n",
    "        self.post = nn.Linear(cfg.num_qubits, n_heads)\n",
    "        self.scale_range = cfg.scale_range\n",
    "\n",
    "        # PQC weights (L, wires, 3)\n",
    "        self.q_weights = nn.Parameter(0.01 * torch.randn(cfg.q_layers, cfg.num_qubits, 3))\n",
    "\n",
    "        # small quantum device (CPU)\n",
    "        self.dev = qml.device(cfg.device_name, wires=cfg.num_qubits)\n",
    "\n",
    "        @qml.qnode(self.dev, interface=\"torch\", diff_method=cfg.diff_method)\n",
    "        def circuit(angles, weights):\n",
    "            qml.templates.AngleEmbedding(angles, wires=range(cfg.num_qubits), rotation=\"Y\")\n",
    "            StronglyEntanglingLayers(weights, wires=range(cfg.num_qubits))\n",
    "            return [qml.expval(qml.PauliZ(i)) for i in range(cfg.num_qubits)]\n",
    "\n",
    "        self.circuit = circuit\n",
    "\n",
    "    def forward(self, summary_vec: torch.Tensor) -> torch.Tensor:\n",
    "        # summary_vec: [B, H]  (hidden size)\n",
    "        B, _ = summary_vec.shape\n",
    "        angles = self.to_angles(summary_vec)  # [B, num_qubits]\n",
    "        outs = []\n",
    "        for b in range(B):\n",
    "            ev = self.circuit(angles[b], self.q_weights)  # [num_qubits]\n",
    "            # ✅ Convert ev into a proper torch tensor before stacking\n",
    "            ev = torch.tensor(ev, dtype=angles.dtype, device=angles.device)\n",
    "            outs.append(ev.unsqueeze(0))\n",
    "\n",
    "        qfeat = torch.cat(outs, dim=0)  # [B, num_qubits]\n",
    "        head_logits = self.post(qfeat)  # [B, n_heads]\n",
    "        # squash to scales in (1 - r, 1 + r)\n",
    "        scales = 1.0 + self.scale_range * torch.tanh(head_logits)\n",
    "        return scales  # [B, n_heads]\n",
    "\n",
    "\n",
    "\n",
    "class GPT2WithQuantumGate(nn.Module):\n",
    "    def __init__(self, model_name: str = \"gpt2\", freeze_lm: bool = True, qcfg: QAGConfig = QAGConfig()):\n",
    "        super().__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.lm = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "        if freeze_lm:\n",
    "            for p in self.lm.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        # discover model dims & heads\n",
    "        hidden = self.lm.config.n_embd\n",
    "        n_layers = self.lm.config.n_layer\n",
    "        n_heads = self.lm.config.n_head\n",
    "\n",
    "        # one quantum gate per block\n",
    "        self.qgates = nn.ModuleList([\n",
    "            QuantumAttentionGate(in_dim=hidden, n_heads=n_heads, cfg=qcfg)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "        self.qcfg = qcfg\n",
    "\n",
    "        # register forward hooks to scale attention outputs per block\n",
    "        self._hooks: List[torch.utils.hooks.RemovableHandle] = []\n",
    "        for i, block in enumerate(self.lm.transformer.h):\n",
    "            handle = block.attn.register_forward_hook(self._make_attn_hook(i))\n",
    "            self._hooks.append(handle)\n",
    "\n",
    "    def _make_attn_hook(self, layer_idx: int):\n",
    "        gate = self.qgates[layer_idx]\n",
    "\n",
    "        def hook(module, inputs, outputs):\n",
    "            # GPT‑2 attention forward returns: attn_output, attn_weights, present, ...\n",
    "            # We want to scale attn_output *per head* using a summary of the *input hidden states*.\n",
    "            # inputs: (hidden_states, layer_past, attention_mask, ...)\n",
    "            hidden_states = inputs[0]  # [B, T, H]\n",
    "            attn_output = outputs[0]   # [B, T, H]\n",
    "\n",
    "            # get per‑head representation by reshaping attn_output\n",
    "            B, T, H = attn_output.shape\n",
    "            n_heads = module.num_heads\n",
    "            head_dim = H // n_heads\n",
    "            attn_heads = attn_output.view(B, T, n_heads, head_dim)\n",
    "\n",
    "            # summary vector from *inputs* (more stable than outputs), e.g., mean over tokens\n",
    "            if self.qcfg.reduce == \"mean\":\n",
    "                summary = hidden_states.mean(dim=1)  # [B, H]\n",
    "            elif self.qcfg.reduce == \"last\":\n",
    "                summary = hidden_states[:, -1, :]\n",
    "            else:  # cls isn't defined for GPT‑2; fallback to mean\n",
    "                summary = hidden_states.mean(dim=1)\n",
    "\n",
    "            # quantum scales: [B, n_heads]\n",
    "            scales = gate(summary)\n",
    "            scales = scales.view(B, 1, n_heads, 1)  # broadcast over tokens and head_dim\n",
    "\n",
    "            gated = attn_heads * scales\n",
    "            gated = gated.view(B, T, H)\n",
    "\n",
    "            # replace attn_output in the tuple\n",
    "            new_outputs = list(outputs)\n",
    "            new_outputs[0] = gated\n",
    "            return tuple(new_outputs)\n",
    "\n",
    "        return hook\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def generate(self, prompt: str, max_new_tokens: int = 128, temperature: float = 0.8,\n",
    "                 top_p: float = 0.95, top_k: int = 0, device: Optional[str] = None) -> str:\n",
    "        if device is None:\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.to(device)\n",
    "        self.lm.eval()\n",
    "\n",
    "        enc = self.tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        out_ids = self.lm.generate(\n",
    "            **enc,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=max(1e-5, temperature),\n",
    "            top_p=top_p,\n",
    "            top_k=top_k,\n",
    "            pad_token_id=self.tokenizer.pad_token_id,\n",
    "            eos_token_id=self.tokenizer.eos_token_id,\n",
    "        )\n",
    "        return self.tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def demo(model_name: str, prompt: str, max_new_tokens: int):\n",
    "    model = GPT2WithQuantumGate(model_name=model_name, freeze_lm=True)\n",
    "    text = model.generate(prompt=prompt, max_new_tokens=max_new_tokens)\n",
    "    print(\"\\n=== OUTPUT (GPT + Quantum Attention Gate) ===\\n\", text)\n",
    "\n",
    "\n",
    "\n",
    "demo(model_name=\"gpt2\", prompt=\"How are you ?\", max_new_tokens=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9eddda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
